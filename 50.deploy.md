# FastAPI Deployment on Ubuntu Server with Docker and Nginx

This guide covers the steps to deploy a FastAPI application running inside a Docker container on an Ubuntu server, using Nginx as a reverse proxy.

---

## 1. Update the system: Ubuntu 24, Python 3.12

```bash
sudo apt update && sudo apt upgrade -y

python3 --version
pip3 --version
sudo apt install python3
sudo apt install python3-pip python3-venv -y

python3 -m venv myenv
source myenv/bin/activate

pip3 install pandas

deactivate

sudo apt install nodejs npm
node -v
npm -v

sudo snap install code --classic


ls -l             # List with details
ls -a             # Show hidden files
cd ~              # Go to home directory
mkdir <folder>    # Create a directory
rm <file>         # Delete a file
rm -r <folder>    # Delete a directory and contents
cp <src> <dest>   # Copy files or folders
mv <src> <dest>   # Move or rename files/folders

top               # Show running processes
htop              # (Install first) Advanced process viewer
df -h             # Disk space usage
free -h           # Memory usage
uname -r          # Kernel version

cat <file>        # View file content
less <file>       # Scroll through file
nano <file>       # Edit file (simple editor)

ping <host>       # Test network connection
ip addr           # Show IP addresses
ifconfig          # (Install net-tools) Show network info
ssh user@host     # Connect via SSH

find <path> -name <filename>    # Find files
grep "<text>" <file>            # Search text inside file

sudo apt install <package>      # Install software
sudo apt remove <package>       # Remove software

chmod <perm> <file>             # Change permissions
chown <user>:<group> <file>     # Change ownership
```
---

## 2. Install Docker and Docker Compose

```bash
sudo apt install docker.io -y
sudo apt install docker-compose
docker --version
docker-compose --version

sudo systemctl start docker
sudo systemctl enable docker          # start with system

docker pull qdrant/qdrant:v1.15       # Download an image
docker images                         # List all local images
docker rmi image_name                 # Remove an image

docker run -it ubuntu                 # Run container interactively
docker run -d nginx                   # Run in detached (background) mode
docker ps                             # List running containers
docker ps -a                          # List all containers (running + stopped)
docker stop container_id              # Stop a running container
docker start container_id             # Start a stopped container
docker rm container_id                # Remove a container

docker inspect container_id           # Inspect detailed info
docker logs container_id              # View container logs
docker exec -it container_id /bin/bash  # Open shell in container

docker container prune                # Remove all stopped containers
docker image prune                    # Remove unused images
docker system prune -a                # Full cleanup: images, containers, volumes, networks

docker volume create myvolume                         # Create volume
docker run -v myvolume:/data -it ubuntu               # Mount volume in container

docker network create mynet                           # Create custom network

docker run hello-world                 # Test Docker installation
```
---

## 3. Create project directory and files

```bash
mkdir ~/fastapi-docker
cd ~/fastapi-docker
```

### Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### requirements.txt

```
fastapi
uvicorn[standard]
```

### main.py

```python
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
async def root():
    return {"message": "Hello from FastAPI in Docker!"}
```

---

## 4. Create docker-compose.yml

```yaml
version: "3.9"

services:
  fastapi:
    build: .
    ports:
      - "8000:8000"
    restart: always
```

---

## 5. Build and run the Docker container

```bash
docker-compose up -d --build
```

Check running containers:

```bash
docker ps
```

Access the app at `http://SERVER_IP:8000`.

---

## 6. Install and configure Nginx as a reverse proxy

```bash
sudo apt install -y nginx
```

Create Nginx site config `/etc/nginx/sites-available/fastapi`:

```nginx
server {
    listen 80;
    server_name your_domain_or_IP;

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Enable the site and reload Nginx:

```bash
sudo ln -s /etc/nginx/sites-available/fastapi /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl reload nginx
```

---

## 7. (Optional) Set up SSL with Let's Encrypt

```bash
sudo apt install certbot python3-certbot-nginx
sudo certbot --nginx -d your_domain
```

---

## 8. Additional Notes

* Docker container runs FastAPI on port 8000 internally.
* Nginx proxies requests from port 80 (HTTP) or 443 (HTTPS) to the FastAPI app.
* To limit file upload size, add `client_max_body_size 10M;` inside the `server` block in the Nginx config.
* You can extend `docker-compose.yml` with other services like databases or caches.

---

## Summary

You now have a FastAPI app running inside Docker, served through Nginx on your Ubuntu server. This setup provides a scalable and secure production-ready environment.

---

Feel free to customize or ask for help to add more features like firewall setup, auto-deploy, or monitoring!

```bash
nvidia-smi

sudo apt update
sudo apt install -y nvidia-driver-535
sudo reboot

python3 -m venv whisper-env
source whisper-env/bin/activate

python3 -c "import torch; print(torch.cuda.is_available())"

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129


pip install faster-whisper

python3 -c "
from faster_whisper import WhisperModel

model = WhisperModel('medium', device='cuda', compute_type='float16')

segments, info = model.transcribe('your-audio-file.wav')
"

# 8. Monitor GPU usage (in another terminal)
watch -n 1 nvidia-smi
```